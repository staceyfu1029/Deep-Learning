{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import numpy as np\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import os\n",
    "\n",
    "PATH = \"D:/study/2020 Spring/597 Deep Learning/dataset/cats_and_dogs_filtered\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "##setting the dir of the datasets; \n",
    "train_dir = \"D:/study/2020 Spring/597 Deep Learning/dataset/cats_and_dogs_filtered/train\"\n",
    "validation_dir = \"D:/study/2020 Spring/597 Deep Learning/dataset/cats_and_dogs_filtered/validation\"\n",
    "\n",
    "train_cats_dir = \"D:/study/2020 Spring/597 Deep Learning/dataset/cats_and_dogs_filtered/train/cats\"  # directory with our training cat pictures\n",
    "train_dogs_dir = \"D:/study/2020 Spring/597 Deep Learning/dataset/cats_and_dogs_filtered/train/dogs\"  # directory with our training dog pictures\n",
    "validation_cats_dir = \"D:/study/2020 Spring/597 Deep Learning/dataset/cats_and_dogs_filtered/validation/cats\"  # directory with our validation cat pictures\n",
    "validation_dogs_dir = \"D:/study/2020 Spring/597 Deep Learning/dataset/cats_and_dogs_filtered/validation/dogs\"  # directory with our validation dog pictures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "Found 2000 images belonging to 2 classes.\n",
      "Found 2000 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "#take a peek at the data; \n",
    "print(len(os.listdir(train_cats_dir)))\n",
    "#load images using ImageDataGenerator; \n",
    "train_image_generator = ImageDataGenerator(rescale=1./255) # Generator for our training data\n",
    "batch_size = 1000\n",
    "epochs = 15\n",
    "IMG_HEIGHT = 150\n",
    "IMG_WIDTH = 150\n",
    "validation_image_generator = ImageDataGenerator(rescale=1./255) # Generator for our validation data\n",
    "train_data_gen = train_image_generator.flow_from_directory(batch_size=batch_size,\n",
    "                                                           directory=train_dir,\n",
    "                                                           shuffle=True,\n",
    "                                                           target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
    "                                                           class_mode='binary')\n",
    "validation_data_gen = validation_image_generator.flow_from_directory(batch_size=batch_size,\n",
    "                                                           directory=train_dir,\n",
    "                                                           shuffle=True,\n",
    "                                                           target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
    "                                                           class_mode='binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras as keras\n",
    "from keras import models\n",
    "from keras import layers\n",
    "sample_training_images, sample_training_labels = next(train_data_gen)\n",
    "sample_validation_images, sample_validation_labels = next(validation_data_gen)\n",
    "sample_training_images = sample_training_images.reshape((1000, 150 * 150 *3))\n",
    "sample_training_images = sample_training_images.astype('float32') / 255\n",
    "sample_validation_images = sample_validation_images.reshape((1000, 150 * 150 *3))\n",
    "sample_validation_images = sample_validation_images.astype('float32') / 255\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Logistic regression\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics\n",
    "\n",
    "clf = LogisticRegression()\n",
    "clf.fit(sample_training_images,sample_training_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.583\n"
     ]
    }
   ],
   "source": [
    "test_predict = clf.predict(sample_validation_images)\n",
    "print(\"Accuracy:\",metrics.accuracy_score(sample_validation_labels, test_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=1.0, break_ties=False, cache_size=200, class_weight=None, coef0=0.0,\n",
       "    decision_function_shape='ovr', degree=3, gamma='scale', kernel='linear',\n",
       "    max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "    tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Optimal separating hyperplane using SVM\n",
    "from sklearn import svm\n",
    "\n",
    "clf = svm.SVC(kernel='linear')\n",
    "clf.fit(sample_training_images,sample_training_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.573\n"
     ]
    }
   ],
   "source": [
    "test_predict = clf.predict(sample_validation_images)\n",
    "print(\"Accuracy:\",metrics.accuracy_score(sample_validation_labels, test_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1000/1000 [==============================] - 0s 437us/step - loss: 0.2494 - acc: 0.5210\n",
      "Epoch 2/5\n",
      "1000/1000 [==============================] - 0s 303us/step - loss: 0.2484 - acc: 0.5500\n",
      "Epoch 3/5\n",
      "1000/1000 [==============================] - 0s 301us/step - loss: 0.2473 - acc: 0.5530\n",
      "Epoch 4/5\n",
      "1000/1000 [==============================] - 0s 299us/step - loss: 0.2459 - acc: 0.5610\n",
      "Epoch 5/5\n",
      "1000/1000 [==============================] - 0s 303us/step - loss: 0.2461 - acc: 0.5500\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x26a0f716ef0>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Logistic regression (binary) using NN\n",
    "\n",
    "netLogiB = models.Sequential() # Sequential feed forward NN\n",
    "netLogiB.add(layers.Dense(1, activation='sigmoid', input_shape=(150*150*3,)))\n",
    "netLogiB.compile(optimizer='rmsprop',loss='mean_squared_error',metrics=['accuracy'])\n",
    "netLogiB.fit(sample_training_images, sample_training_labels, epochs=5, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 0s 362us/step\n",
      "0.582\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = netLogiB.evaluate(sample_validation_images, sample_validation_labels)\n",
    "print(test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical\n",
    "sample_training_labels = to_categorical(sample_training_labels)\n",
    "sample_validation_labels = to_categorical(sample_validation_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Stacey\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3295: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Stacey\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow_core\\python\\ops\\math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "Epoch 1/5\n",
      "1000/1000 [==============================] - 1s 516us/step - loss: 0.6956 - acc: 0.5010\n",
      "Epoch 2/5\n",
      "1000/1000 [==============================] - 0s 400us/step - loss: 0.6937 - acc: 0.5540\n",
      "Epoch 3/5\n",
      "1000/1000 [==============================] - 0s 396us/step - loss: 0.6903 - acc: 0.5500\n",
      "Epoch 4/5\n",
      "1000/1000 [==============================] - 0s 391us/step - loss: 0.6880 - acc: 0.5420\n",
      "Epoch 5/5\n",
      "1000/1000 [==============================] - 0s 384us/step - loss: 0.6876 - acc: 0.5210\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x26a0f54c550>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Logistic regression (multinomial) using NN\n",
    "\n",
    "netLogiM = models.Sequential() # Sequential feed forward NN\n",
    "netLogiM.add(layers.Dense(2, activation='sigmoid', input_shape=(150*150*3,)))\n",
    "netLogiM.compile(optimizer='rmsprop',loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "netLogiM.fit(sample_training_images, sample_training_labels, epochs=5, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 0s 312us/step\n",
      "0.524\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = netLogiM.evaluate(sample_validation_images, sample_validation_labels)\n",
    "print(test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1000/1000 [==============================] - 1s 536us/step - loss: 8.1250 - acc: 0.4970\n",
      "Epoch 2/5\n",
      "1000/1000 [==============================] - 0s 400us/step - loss: 8.2202 - acc: 0.4970\n",
      "Epoch 3/5\n",
      "1000/1000 [==============================] - 0s 396us/step - loss: 8.2202 - acc: 0.4970\n",
      "Epoch 4/5\n",
      "1000/1000 [==============================] - 0s 396us/step - loss: 8.2202 - acc: 0.4970\n",
      "Epoch 5/5\n",
      "1000/1000 [==============================] - 0s 404us/step - loss: 8.2202 - acc: 0.4970\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x26a0f5e6f60>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Linear regression using NN\n",
    "\n",
    "netLin = models.Sequential() # Sequential feed forward NN\n",
    "netLin.add(layers.Dense(2, activation='linear', input_shape=(150*150*3,)))\n",
    "netLin.compile(optimizer='rmsprop',loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "netLin.fit(sample_training_images, sample_training_labels, epochs=5, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 0s 321us/step\n",
      "0.509\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = netLin.evaluate(sample_validation_images, sample_validation_labels)\n",
    "print(test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1000/1000 [==============================] - 1s 656us/step - loss: 0.8609 - acc: 0.5230\n",
      "Epoch 2/5\n",
      "1000/1000 [==============================] - 0s 443us/step - loss: 0.6838 - acc: 0.4970\n",
      "Epoch 3/5\n",
      "1000/1000 [==============================] - 0s 441us/step - loss: 0.6672 - acc: 0.5340\n",
      "Epoch 4/5\n",
      "1000/1000 [==============================] - 0s 422us/step - loss: 0.6394 - acc: 0.5030\n",
      "Epoch 5/5\n",
      "1000/1000 [==============================] - 0s 414us/step - loss: 0.6391 - acc: 0.5050\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x26a0f8ece10>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Optimal separating hyperplane using NN\n",
    "\n",
    "netSplane = models.Sequential()\n",
    "netSplane.add(layers.Dense(2, activation='linear', kernel_regularizer=keras.regularizers.l2()))\n",
    "\n",
    "# Train the model\n",
    "netSplane.compile(optimizer='adam', loss=\"hinge\",metrics=['accuracy'])\n",
    "netSplane.fit(sample_training_images, sample_training_labels, epochs=5, batch_size=128)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 0s 379us/step\n",
      "0.509\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = netSplane.evaluate(sample_validation_images, sample_validation_labels)\n",
    "print(test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
