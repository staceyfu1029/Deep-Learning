{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import numpy as np\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import os\n",
    "\n",
    "PATH = \"D:/study/2020 Spring/597 Deep Learning/dataset/cats_and_dogs_filtered\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "##setting the dir of the datasets; \n",
    "train_dir = \"D:/study/2020 Spring/597 Deep Learning/dataset/cats_and_dogs_filtered/train\"\n",
    "validation_dir = \"D:/study/2020 Spring/597 Deep Learning/dataset/cats_and_dogs_filtered/validation\"\n",
    "\n",
    "train_cats_dir = \"D:/study/2020 Spring/597 Deep Learning/dataset/cats_and_dogs_filtered/train/cats\"  # directory with our training cat pictures\n",
    "train_dogs_dir = \"D:/study/2020 Spring/597 Deep Learning/dataset/cats_and_dogs_filtered/train/dogs\"  # directory with our training dog pictures\n",
    "validation_cats_dir = \"D:/study/2020 Spring/597 Deep Learning/dataset/cats_and_dogs_filtered/validation/cats\"  # directory with our validation cat pictures\n",
    "validation_dogs_dir = \"D:/study/2020 Spring/597 Deep Learning/dataset/cats_and_dogs_filtered/validation/dogs\"  # directory with our validation dog pictures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "Found 2000 images belonging to 2 classes.\n",
      "Found 2000 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "#take a peek at the data; \n",
    "print(len(os.listdir(train_cats_dir)))\n",
    "#load images using ImageDataGenerator; \n",
    "train_image_generator = ImageDataGenerator(rescale=1./255) # Generator for our training data\n",
    "batch_size = 1000\n",
    "epochs = 15\n",
    "IMG_HEIGHT = 150\n",
    "IMG_WIDTH = 150\n",
    "validation_image_generator = ImageDataGenerator(rescale=1./255) # Generator for our validation data\n",
    "train_data_gen = train_image_generator.flow_from_directory(batch_size=batch_size,\n",
    "                                                           directory=train_dir,\n",
    "                                                           shuffle=True,\n",
    "                                                           target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
    "                                                           class_mode='binary')\n",
    "validation_data_gen = validation_image_generator.flow_from_directory(batch_size=batch_size,\n",
    "                                                           directory=train_dir,\n",
    "                                                           shuffle=True,\n",
    "                                                           target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
    "                                                           class_mode='binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow.keras as keras\n",
    "from keras import models\n",
    "from keras import layers\n",
    "sample_training_images, sample_training_labels = next(train_data_gen)\n",
    "sample_validation_images, sample_validation_labels = next(validation_data_gen)\n",
    "sample_training_images = sample_training_images.reshape((1000, 150 * 150 *3))\n",
    "sample_training_images = sample_training_images.astype('float32') / 255\n",
    "sample_validation_images = sample_validation_images.reshape((1000, 150 * 150 *3))\n",
    "sample_validation_images = sample_validation_images.astype('float32') / 255\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics\n",
    "\n",
    "clf = LogisticRegression()\n",
    "clf.fit(sample_training_images,sample_training_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.59\n"
     ]
    }
   ],
   "source": [
    "test_predict = clf.predict(sample_validation_images)\n",
    "print(\"Accuracy:\",metrics.accuracy_score(sample_validation_labels, test_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=1.0, break_ties=False, cache_size=200, class_weight=None, coef0=0.0,\n",
       "    decision_function_shape='ovr', degree=3, gamma='scale', kernel='linear',\n",
       "    max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "    tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "\n",
    "clf = svm.SVC(kernel='linear')\n",
    "clf.fit(sample_training_images,sample_training_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.569\n"
     ]
    }
   ],
   "source": [
    "test_predict = clf.predict(sample_validation_images)\n",
    "print(\"Accuracy:\",metrics.accuracy_score(sample_validation_labels, test_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Stacey\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Stacey\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Stacey\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Stacey\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Stacey\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow_core\\python\\ops\\math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From C:\\Users\\Stacey\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Stacey\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:973: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Stacey\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:2741: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Stacey\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Stacey\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Stacey\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:190: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Stacey\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:199: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Stacey\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:206: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1be80046390>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "network3 = models.Sequential()\n",
    "network3.add(layers.Dense(1, activation='linear', kernel_regularizer=keras.regularizers.l2()))\n",
    "\n",
    "sample_training_labels_2 = np.where(sample_training_labels==1, 1, -1)\n",
    "# Hinge loss\n",
    "def hinge_loss(y_true, y_pred):    \n",
    "    return tf.maximum(0., 1- y_true*y_pred)\n",
    "\n",
    "# Train the model\n",
    "network3.compile(optimizer='adam', loss=hinge_loss)\n",
    "network3.fit(sample_training_images, sample_training_labels_2, epochs=5, verbose=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = network3.predict(sample_validation_images)\n",
    "y_pred = np.where(y_pred>0,1,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.493\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy:\",metrics.accuracy_score(sample_validation_labels, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical\n",
    "sample_training_labels = to_categorical(sample_training_labels)\n",
    "sample_validation_labels = to_categorical(sample_validation_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Stacey\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3295: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "Epoch 1/5\n",
      "1000/1000 [==============================] - 1s 535us/step - loss: 0.6979 - acc: 0.4820\n",
      "Epoch 2/5\n",
      "1000/1000 [==============================] - 0s 409us/step - loss: 0.6921 - acc: 0.5190\n",
      "Epoch 3/5\n",
      "1000/1000 [==============================] - 0s 390us/step - loss: 0.6902 - acc: 0.5230\n",
      "Epoch 4/5\n",
      "1000/1000 [==============================] - 0s 394us/step - loss: 0.6878 - acc: 0.5370\n",
      "Epoch 5/5\n",
      "1000/1000 [==============================] - 0s 400us/step - loss: 0.6864 - acc: 0.5660\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1be800e23c8>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "network = models.Sequential() # Sequential feed forward NN\n",
    "network.add(layers.Dense(2, activation='sigmoid', input_shape=(150*150*3,)))\n",
    "network.compile(optimizer='rmsprop',loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "network.fit(sample_training_images, sample_training_labels, epochs=5, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 0s 310us/step\n",
      "0.582\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = network.evaluate(sample_validation_images, sample_validation_labels)\n",
    "print(test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1000/1000 [==============================] - 1s 522us/step - loss: 7.3492 - acc: 0.5010\n",
      "Epoch 2/5\n",
      "1000/1000 [==============================] - 0s 406us/step - loss: 8.1558 - acc: 0.4960\n",
      "Epoch 3/5\n",
      "1000/1000 [==============================] - 0s 401us/step - loss: 8.1558 - acc: 0.4960\n",
      "Epoch 4/5\n",
      "1000/1000 [==============================] - 0s 400us/step - loss: 8.1558 - acc: 0.4960\n",
      "Epoch 5/5\n",
      "1000/1000 [==============================] - 0s 402us/step - loss: 8.1558 - acc: 0.4960\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1be8018d208>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "network2 = models.Sequential() # Sequential feed forward NN\n",
    "network2.add(layers.Dense(2, activation='linear', input_shape=(150*150*3,)))\n",
    "network2.compile(optimizer='rmsprop',loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "network2.fit(sample_training_images, sample_training_labels, epochs=5, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 0s 324us/step\n",
      "0.507\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = network2.evaluate(sample_validation_images, sample_validation_labels)\n",
    "print(test_acc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
